---
title: 概率论和数理统计
date: 2024-12-04 19:05:12
updated: "04 Dec 2024"
publishDate: "04 Dec 2024"
description: "这篇文章介绍了概率论和数理统计的基本概念，包括常见的分布及其性质、随机变量的分布函数、数字特征等内容"
tags: ["概率论", "数理统计", "随机变量", "分布函数", "数学期望", "方差", "协方差", "相关系数"]

---

# 常见的分布及其性质

## 泊松分布(Poisson Distribution)

$$
X\sim P(\lambda)\\
X\sim \pi(\lambda)
\\P(X=k)=\frac{\lambda^k e^{-\lambda}}{k!},k=0,1,2
$$

**性质**：

- 数学期望和方差都等于参数$\lambda$

- $\lambda > 0$

- 可加性：

  若随机变量 $X\sim\pi(\lambda_1),Y\sim\pi(\lambda_2)$，且$X,Y$相互独立。若$Z=X+Y$，则$Z\sim\pi(\lambda_1+\lambda_2)$

## 均匀分布(Uniform Distribution)

$$
X\sim U(a,b)
$$

$X$具有概率密度
$$
f(x)=
\begin{cases}
\frac{1}{b-a},x\in(a,b)\\
0,\text{else}
\end{cases}
$$

方差为
$$
D(X)=\frac{(a-b)^2}{12}
$$


## 指数分布(Exponential Distribution)

$$
X\sim E(\lambda)
$$

$X$服从参数为$\lambda$的指数分布时，$X$具有概率密度
$$
f(x)=
\begin{cases}
\lambda e ^{-\lambda x},x>0\\
0,\text{else}
\end{cases}
$$
$X$的分布函数
$$
F(x)=
\begin{cases}
1-e^{-\lambda x},x >0\\
0,\text{else}
\end{cases}
$$
服从指数分布的随机变量$X$具有**无记忆性**，即满足
$$
P(X>t_0+t|X>t_0)=P(X>t)
$$

**数学期望和方差**：
$$
E(X)=\frac{1}{\lambda}\\D(X)=\frac{1}{\lambda^2}
$$


## 正态分布

$$
X\sim N(\mu,\sigma^2)
$$

服从正态分布的随机变量的概率密度函数满足
$$
f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}},-\infty<x<+\infty
$$
若$Z\sim N(0,1)$，称$Z$服从标准正态分布，$Z$的概率密度：
$$
\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
$$
 **普通正态分布转换为标准正态分布的计算公式**

当$X\sim N(\mu,\sigma^2)$时，$\frac{X-\mu}{\sigma}\sim N(0,1)$
$$
P(x\leq b)=\Phi(\frac{b-\mu}{\sigma})
$$
$\mu,\sigma$分别是普通正态的均值和标准差。

## 几何分布(Geometric Distribution)

几何分布指的是在一系列伯努利试验中，**第一次成功出现之前的失败次数为k-1**时的概率
$$
P(X=k)=(1-p)^{k-1}p
$$
**性质**

- 期望
  $$
   E(X)=\frac{1-p}{p}
  $$

- 方差
  $$
  D(X)=\frac{1-p}{p^2}
  $$

# $\Gamma$函数求积分

$\Gamma$函数的表达如下
$$
\Gamma(z)=\int_o^{\infty} t^{z-1}e(-t)dt
$$
对于$z$为整数的时候，可以直接进行积分

- $$
  \Gamma(\frac{1}{2})=\sqrt\pi
  $$

- $$
  \Gamma(n)=(n-1)!
  $$


# 随机变量的分布函数

**定义**：$X$为一随机变量,对任意实数$x$,函数$F(x)=P(X\leq x)$为随机变量$X$的概率分布函数，简称分布函数，任何随机变量都有相应的分布函数

#### 离散型随机变量及其分布函数

一般地，离散型随机变量的分布律为$P\{X=x_k\}=p_k$,由此可得$X$的分布函数为$F(x)=\sum_{x_k\leq x}p_k$,分布函数再$x=x_k$处有跳跃，跳跃值为$p_k=P\{X=x_k\}$

#### 连续型随机变量及其分布函数

定义：对于随机变量$X$的分布函数$F(x)$，若存在非负函数$f(x)$，使得对于任意实数$x$，有：
$$
F(x)=\int_{-\infty}^xf(t)dt
$$
则称$X$为连续型随机变量，$f(x)$称为$X$的概率密度函数。

## 二元随机变量

#### 离散型二元随机变量

**定义**：若二元随机变量$(X,Y)$全部可能取到的不同值是有限对或可列无限对，则称$(X,Y)$是离散型二元随机变量。二元随机变量的分布律如下：
$$
P(X=x_i,Y=y_i)=p_{ij}
$$

#### 边际分布

对于上述的离散型随机变量，$X,Y$的边际分布律为：
$$
P(Y=y_i)=P(X<+\infty,Y=y_i)=\sum_{i=1}^\infty p_{ij}\\
P(X=x_i)=P(X=x_i,Y<+\infty)=\sum_{j=1}^\infty p_{ij}\\
$$

#### 条件分布

对于二元离散型随机变量$X,Y$，若$P(Y=y_j)=p_{·j}>0$，那么条件概率
$$
P(X=x_i|Y=y_j)=\frac{P(X=x_i,Y=y_j)}{P(Y=y_j)}=\frac{P_{ij}}{P_{·j}}
$$
为在 $Y=y_j$条件下，随机变量$X$的条件分布律。

### 二元随机变量的分布函数

**定义**：设$(X,Y)$是二元随机变量，对于任意实数$x,y$，二元函数
$$
F(x,y)=p\{(X\leq x)\cap (Y\leq y)\}=P(X\leq x,Y\leq y)
$$
称为二元随机变量$(X,Y)$的分布函数

**边际分布函数**

对二元分布函数进行积分，对$x$进行积分得到的是$y$的边际分布函数，反之亦然
$$
F_X(x)=F(x,\infty)\\
F_Y(y)=F(\infty,y)
$$
**条件分布函数**

若$P(Y=y)>0$，那么在$Y=y$的条件下，$X$的分布函数为：
$$
F_(X|Y)(x|y)=P(X\leq x|Y=y)=\frac{P(X\leq x,Y=y)}{P(Y=y)}
$$
若$P(Y=y)=0$，但对任意给定的$\epsilon >0$，都有$P(y<Y\leq y+\epsilon)>0$，则在$Y=y$的条件下，$X$的分布函数为：
$$
F_{X|Y}(x|y)=\lim_{\epsilon\rightarrow 0^+}{P(X\leq x|y<Y\leq y+\epsilon})\\
=\lim_{\epsilon\rightarrow 0^+}\frac{P(X=x,y<Y\leq \epsilon+y)}{P(y<Y\leq\epsilon+y)}
$$
也还是记作
$$
P(X\leq x|Y=y)
$$
**联合概率密度**

对于二元随机变量$(X,Y)$的分布函数  $F(x,y)$，如果存在非负函数$f(x,y)$，使对于任意$x,y$有
$$
F(x,y)=\int_{-\infty}^x\int_{-\infty}^yf(u,v)dvdu
$$
称$(X,Y)$为二元连续型随机变量，称$f(x,y)$为二元随机变量$(X,Y)$的联合概率密度

在$f(x,y)$的连续点$(x,y)$，有
$$
\frac{\partial^2F(x,y)}{\partial x\partial y}=f(x,y)
$$
**边际概率密度**

对于连续型随机变量$(X,Y)$，联合概率密度为$f(x,y)$，边际概率密度为
$$
f_X(x)=\int_{-\infty}^{+\infty}f(x,y)dy\\
f_Y(y)=\int_{-\infty}^{+\infty}f(x,y)dx
$$
**条件概率密度**

对于连续型随机变量$(X,Y)$，联合概率密度为$f(x,y)$，$(X,Y)$关于$Y$的边际概率密度为$f_Y(y)$，若对于固定的$y$，$f_Y(y)>0$，则
$$
\frac{f(x,y)}{f_Y(y)}
$$
为在$Y=y$的条件下，$X$ 的条件概率密度，记为
$$
f_{X|Y}(x,y)=\frac{f(x,y)}{f_Y(y)}
$$
对$X=x$的条件下，$Y$的条件概率密度，记为
$$
f_{Y|X}(x,y)=\frac{f(x,y)}{f_X(x)}
$$

### 随机变量的独立性

设$F(x,y),F_X(),F_Y(y)$分别是二元随机变量 $(X,Y)$的分布函数和边际分布函数，若对所有的$(x,y)$都有$P(X\leq x,Y\leq y)=P(X\leq x)P(Y\leq y)$即
$$
F(x,y)=F_X(x)F_Y(y)
$$
称随机变量$X,Y$相互独立

- 若随机变量$X,Y$是离散型的，那么$p_{ij}=p_ip_j$对一切$i,j$都成立
- 若随机变量$X,Y$是连续型的，那么$f(x,y)=f_X(x)f_Y(y)$处处成立

### 随机变量函数的分布

问题：已知随机变量$X$的分布，且已知$Y=g(x)$，求$Y$的概率分布。

一般，上述问题的求解方法为：等价变换法

- 对于离散型随随机变量$X$，只需找到 $Y$的等价事件即可，即先写出$Y$的可能取值，再找出$(Y=y_j)$的等价事件$X_i$，然后进行概率对应
- 若为离散型的随机变量$X$，那么先写出$Y$的概率分布函数$F_Y(y)=P(Y\leq y)$，找到$Y\leq y$的等价时间$X\in D$,得到$F_Y(y)=P(X\in D)$，再求出$Y$的概率密度函数$f_Y(y)$

**定理 反函数法**

设$X\sim f_X(x),-\infty<x<+\infty,g^{'}(x)>0\text{或}(g^{'}(x)<0),Y=g(X)$,则 $Y$具有概率密度函数为
$$
f_Y(y)=
\begin{cases}
f_X(h(y)) ·|h^{'}(y)|,\alpha<y<\beta\\
0,\text{else}
\end{cases}
$$
这里 $\alpha,\beta$是$Y$的取值范围，$h$是$g$的反函数$h(y)=x$.

### 二元随机变量函数的分布

问题：设二元离散型随机变量$(X,Y)$具有概率分布$P(X=x_i,Y=y_j)=p_{ij}$，设$U=u(X,Y),V=v(X,Y)$，则 $(U,V)$的分布律是什么，$Z=g(X,Y)$的分布律是什么？

- 对于 $U,V,Z$为离散型的情况，只要找到对应的等价事件就可以了

#### $Z=X+Y$的分布

- 设$(X,Y)$为离散型随机变量，分布律为：
  $$
  P(X=x_i,Y=y_j)=p_{ij}
  $$
   设$Z$的可能取值为$z_1,z_2,...,z_k,...$，则$Z=X+Y$的分布律为
  $$
  P(Z=z_k)=P(X+Y=z_k)\\
  =\sum_{i=1}^{+\infty}P(X=x_i,Y=z_k-x_i)
  $$
  当$X,Y$相互独立时
  $$
  P(Z=z_k)=\sum_{i=1}^{+\infty}p(X=x_i)P(Y=z_k-x_i)
  $$

- 设$(X,Y)$为连续型随机变量，概率密度为 $f(x,y)$，$F_Z(z)=P(Z\leq z)=\iint_{x+y\leq z}dxdy$.可以得到$Z$ 的概率密度为
  $$
  f_Z(z)=\int_{-\infty}^{+\infty}f(z-y,y)dy
  $$
  由$X,Y$的对称性，也可以写为
  $$
  f_Z(z)=\int_{-\infty}^{\infty}f(x,z-x),dx
  $$
  当$X,Y$相互独立时， $Z=X+Y$的密度函数公式称为卷积公式
  $$
  f_Z(z)=\int_{-\infty}^{+\infty}f_X(z-y)f_Y(y)dy=\int_{-\infty}^{+\infty}f_X(x)f_Y(z-x)dx
  $$

### $M=\text{max}(X,Y),N=\text{min}(X,Y)$的分布

设$X,Y$是两个相互独立的随机变量，它们的分布函数分别为$F_X(x),F_Y(y)$，现在求$M,N$的分布函数$F_{\text{max}}(z),F_{\text{min}}(z)$.
$$
F_{\text{max}}(z)=F_X(z)F_Y(z)\\
F_{\text{min}}(z)=1-P(N>z)=1-P(X>z,Y>z)=1-(1-F_X(z))(1-F_Y(z))
$$
可以推广到$n$个相互独立的随机变量的情况

计算$\max(X_1,X_2,...,X_n)$的分布函数：

1. 算出一个随机变量$X$的分布函数

2. 用一个随机变量的分布函数$F(x)$计算整体的分布函数

   - $$
     F_{\max(X_1,X_2,...,X_n)}(x)=(F(x))^N
     $$

   - $$
     F_{\min(X_1,X_2,...,X_n)}(x)=1-(1-F(x))^n
     $$

     

# 随机变量的数字特征

### 数学期望

对离散型随机变量$X$，若级数
$$
\sum_{k=1}^\infty x_kp_k
$$
绝对收敛，那么该级数的值就是随机变量$X$的数学期望，记为$E(X)$
$$
E(X)=\sum_{k=1}^\infty x_kp_k
$$
对于连续型随机变量，若积分
$$
\int_{-\infty}^{+\infty}xf(x)dx
$$
绝对收敛，则该积分的值就是随机变量$X$的数学期望，记为$E(X)$
$$
E(X)=\int_{-\infty}^{+\infty}xf(x)dx
$$
对于随机变量$Y=g(X)$，有定理(该定理也可推广到两个或两个以上的随机变量的函数的情况)
$$
E(Y) = \int_{-\infty}^{+\infty}g(x)f(x)dx
$$
设 $Z=h(X,Y)$，$X,Y$为连续型随机变量（离散型同理累加即可）， $Z$的数学期望为
$$
E(Z)=\int_{-\infty}^{+\infty}h(x,y)f(x,y)dxdy
$$
同时还有
$$
E(X)=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}xf(x,y)dxdy\\
E(Y)=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}yf(x,y)dxdy
$$

#### 数学期望的性质

- $$
  E(C)=C,C为常数
  $$

- $$
  E(CX)=CE(X),X为随机变量，C为常数
  $$

- $$
  E(X+Y)=E(X)+E(Y),X,Y是两个随机变量（不一定相互独立）
  $$

- $$
  E(XY)=E(X)E(Y),X,Y是相互独立的随机变量
  $$

上述第三和第四两个定理都可以推广到任意有限个的情况

### 方差

$X$是一个随机变量，若 $E\{[X-E(X)]^2\}$存在，则称其为$X$的方差，记为$D(X)$或$Var(X)$，即
$$
D(X)=Var(X)=E\{[X-E(X)]^2\}
$$
对于离散型随机变量
$$
D(X)=\sum_{i=1}^\infty[x_i-E(X)]^2*p_i
$$
对于连续型随机变量
$$
D(X)=\int_{-\infty}^{+\infty}[x-E(X)]^2f(x)dx
$$
方差计算公式
$$
D(X)=E(X^2)-[E(X)]^2
$$
**方差的性质**

- $$
  D(C)=0,C为常数
  $$

- $$
  D(CX)=C^2D(X),X为随机变量，C为常数
  $$

- $$
  D(X+Y)=D(X)+D(Y)+2E{[X-E(X)][Y-E(Y)]}\\若X,Y相互独立，则有D(X+Y)=D(X)+D(Y)
  $$

### 协方差与相关系数

两个随机变量的和的方差计算公式中的
$$
E\{[X-E(X)][Y-E(Y)]\}
$$
称为随机变量$X,Y$的协方差。记为
$$
\text{Cov(X,Y)}
$$
即
$$
\text{Cov}(X,Y)=E\{[X-E(X)][Y-E(Y)]\}
$$
定义随机变量$X$与$Y$的相关系数$\rho_{XY}$为
$$
\rho_{XY}=\frac{\text{Cov}(X,Y)}{\sqrt{D(X)D(Y)}}
$$
协方差的计算公式
$$
\text{Cov}(X,Y)=E(XY)-E(X)E(Y)
$$
方差计算公式的重写
$$
D(X+Y)=D(X)+D(Y)+2\text{Cov}(X,Y)
$$
推广到任意有限个随机变量的和的情况
$$
D(\sum_{i=1}^{n}X_i)=\sum_{i=1}^{n}D(X_i)+2\sum_{1\leq i\leq j\leq n}\text{Cov}(X_i,X_j)
$$
**协方差的性质**

- $$
  \text{Cov}(X,Y)=\text{Cov}(Y,X)
  $$

- $$
  \text{Cov}(X,X)=D(X)
  $$

- $$
  \text{Cov}(aX,bY)=ab·\text{Cov}(X,Y),其中a，b为实数
  $$

- $$
  \text{Cov}(X_1+X_2,Y)=\text{Cov}(X_1,Y)+\text{Cov}(X_2,Y)
  $$

**相关系数的性质**

- $$
  |\rho_{XY}|\leq1
  $$

- $|\rho_{XY}|$等价于存在常数$a,b$使得$P(Y=a+bX)=1$,特别的，$\rho_{XY}=1,b>0$,$\rho_{XY}=-1,b<0$

- 当$D(X)D(Y)\neq0$时，有
  $$
  (\text{Cov}(X,Y))^2\leq D(X)D(Y)
  $$
  其中等号当且仅当$X$与$Y$之间有严格的线性关系，即存在常数$a,b$使得$p(Y=a+bX)=1$

**标准化随机变量**

对于任意随机变量$X$,作
$$
 X^*=\frac{X-E(X)}{\sqrt{D(X)}}
$$
则
$$
E(X^*)=0,D(X^*)=1
$$
$X^*$称为标准化随机变量
$$
\rho_{XY}=\text{Cov}(\frac{X-E(X)}{\sqrt{D(X)}},\frac{Y-E(Y)}{\sqrt{D(Y)}})
$$
**不相关**

$\rho_{XY}=0$，称 $X$与$Y$不相关，等价条件有

1. $$
   \text{Cov}（X，Y)=0
   $$

2. $$
   E(XY)=E(X)E(Y)
   $$

3. $$
   D(X+Y)=D(X)D(Y)
   $$

$X,Y$相互独立，那么$X,Y$一定不相关，反之，若$X,Y$不相关，$X,Y$却不一定相互独立

### 二元正态变量

若二元随机变量$X(X,Y)$服从二元正态分布，则$X,Y$的任意线性组合$aX+bY$服从一元正态分布，其中$a,b$为不全为0的常数。且
$$
aX+bY\sim N(E(aX+bY),D(aX+bY))
$$
**正态变量的线性变换不变性**

若$(X,Y)$服从二元正态分布，设$Z_1,Z_2$是$X,Y$的线性函数，则$(Z_1,Z_2)$也服从二元正态分布，这一性质称为正态变量的线性变换不变性
$$
(Z_1,Z_2)\sim N(E(Z_1),D(Z_1),E(Z_2),D(Z_2),\rho_{Z_1Z_2})
$$

# 大数定律和中心极限定律

## 大数定律

设$X_1,...X_n$是一系列随机变量，对于任意一个$X_i$都有$E(X_i)=\mu$,则在一定条件下，随机变量序列
$$
Y_n=\frac{X_1+...+X_n}{n}
$$
收敛到$\mu$。

**随机变量序列依概率收敛的定义**

对于随机变量序列$Y_1,Y_2,...$，若存在某常数$a$,使得$\forall \epsilon >0$都有$\lim_{n\rightarrow\infty}P\{|Y_n-a|\geq\epsilon\}=0$,则称$Y_n$依概率收敛到常数a。

**性质**

若随机变量序列$X_n,Y_n$分别依概率收敛到$a,b$，那么对于一个连续函数映射$g$，就会满足
$$
g(X_n,Y_n)\xrightarrow{P}g(a,b)
$$

### 切比雪夫不等式

设随机变量$X$具有数学期望$E(X)=\mu$,方差$D(X)=\sigma^2$，则对于任意的$\epsilon>0$,都有
$$
P\{|X-E(X)|\geq\epsilon\}\leq\frac{\sigma^2}{\epsilon^2}
$$
等价形式为
$$
P\{|X-E(X)|<\epsilon\}\geq1-\frac{\sigma^2}{\epsilon^2}
$$

### 切比雪夫大数定律

设随机变量$X_1,X_2,...X_n$相互独立，并且有相同的数学期望 $\mu$和相同的方差$\sigma^2$,当$n\rightarrow +\infty$时
$$
\frac{1}{n}\sum_{k=1}^{n}X_k\xrightarrow{P}\mu
$$

### 辛钦大数定律

设$X_1,X_2,...X_n$独立同分布，$E(X_i)=\mu$,则当$n\rightarrow+\infty$
$$
\frac{1}{n}\sum_{k=1}^{n}X_k\xrightarrow{P}\mu
$$

### 伯努利大数定律

设$n_A$为$n$重伯努利实验中$A$发生的次数，并记事件$A$在每次试验中发生的概率为$p$,则有
$$
\frac{n_A}{n}\xrightarrow{P}p,n\rightarrow+\infty
$$

## 中心极限定理

**独立同分布的中心极限定理**

设$X_1,X_2,...,X_n$独立同分布，$E(X_i)=\mu,D(X_i)=\sigma^2$,则当$n$足够大时
$$
\sum_{i=1}^{n}\sim N(n\mu,n\sigma^2)\\\frac{1}{n}\sum_{i=1}^n\sim N(\mu,\frac{\sigma^2}{n})
$$

# 数理统计

## 基础概念

总体：研究对象的全体

个体：总体中的成员

数理统计的主要任务是从总体中抽取一部分个体，然后根据这部分个体的数据对总体分布进行判断，被抽取的部分个体叫做总体的一个样本

随机样本：从总体中随机地取n个个体，称为一个随机样本。

简单随机样本：满足以下两个条件的随机样本($X_1$,$X_2$,...,$X_n$)称为容量是n的简单随机样本。（一般提到的样本都是简单随机样本）

1. 每个$X_i$与$X$同分布
2. $X_1$,$X_2$,...,$X_n$是相互独立的随机变量

由概率论可知，如果总体具有概率函数$f(x)$,那么简单随机样本具有联合密度函数

由于每一个$X_i$与$X$同分布，所以具有相同的概率密度函数，又因为每一个$X_i$相互独立，所以可以直接乘起来得到联合密度函数
$$
f_n(x_1,X_2,...x_n)=\prod_{i=1}^nf(x_i)
$$
**一个**容量为n的样本$X_1$,$X_2$,...,$X_n$是值n个独立并且与总体分布相同的随机变量，一旦对样本进行观察，得到实数$x_1$,$x_2$...$x_n$,称为样本的观察值，两次观察，样本值可能是不同的。

**如何进行取样**

对于有限的总体，采取放回抽样得到简单随机样本，对于总体容量很大或者无限总体采取不放回抽样得到简单随机样本。

统计量：样本的不含任何未知参数的**函数**

设$X_1$,$X_2$,...,$X_n$为取自总体$X$的样本，常用的统计量如下

样本方差$S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2$，$S$为样本标准差
$$
E(\overline{X})=E(X)
$$

$$
D({\overline{X}})=\frac{D(X)}{n}
$$

$$
E(S^2)=D(X)
$$

**样本距**

- $k$阶距：
  $$
  A_k=\frac{1}{n}\sum_{i=1}^nX_i^k,(k=1,2,...)
  $$

- $k$阶中心距
  $$
  B_k=\frac{1}{n}\sum_{i=1}^{n}(X_i-\overline{X})^k,(k=1,2,...)
  $$

这些统计量都是作为总体数字特征未知时的估计。

## 常见的分布

### $\chi^2$分布

设随机变量$X_1$,$X_2$,...$X_n$相互独立，且对于每一个$X_i \sim N(0,1)(i=1,2,...)$，则称
$$
\chi_n^2=\sum_{i=1}^nX_i^2
$$
服从自由度为$n$的$\chi^2$分布，记作
$$
\chi^2\sim \chi^2(n)
$$
自由度指的是独立变量的个数。

**$\chi^2$分布的性质**

1. 设$\chi^2\sim\chi^2(n)$,那么
   $$
   E(\chi^2) = n,D(\chi^2)=2n
   $$

2. 设$Y_1\sim\chi^2(n_1)$,$Y_2\sim\chi^2(n_2)$,且$Y_1$,$Y_2$相互独立，那么
   $$
   Y_1+Y_2\sim\chi^2(n_1+n_2)
   $$

性质2为$\chi^2$分布的可加性，可推广到有限个的一般情况。

对于一个给定的概率$\alpha$,$0<\alpha<1$,称满足条件$\int_{\chi_{\alpha}^{2}(n)}^{\infty}f_n(y)dy=\alpha$的点$\chi_{\alpha}^{2}(n)$为$\chi^2(n)$分布上的$\alpha$分位数

## $t$分布

设$X\sim N(0,1)$,$Y\sim \chi^2(n)$，并且假设$X$,$Y$相互独立，则称随机变量$T=\frac{X}{\sqrt{\frac{Y}{n}}}$服从自由度为$n$的$t$分布，记为
$$
T\sim t(n)
$$
对于给定的$\alpha$,$0<\alpha<1$,称满足$\int_{t_{\alpha}(n)}^{\infty}f(t,n)dt=\alpha$的点$t_{\alpha}(n)$为$t(n)$分布的$\alpha$分位数。
$$
t_{1-\alpha}(n)=-t_\alpha(n)
$$

## $F$分布

设$X\sim \chi^2(n_1)$,$Y\sim \chi^2(n_2)$,且$X$,$Y$独立，则称随机变量$F=\frac{\frac{X}{n_1}}{\frac{Y}{n_2}}$服从自由度为$(n_1,n_2)$的$F$分布，记为
$$
F\sim F(n_1,n_2)
$$
对于给定的$\alpha$,$0<\alpha<1$,称满足条件$\int_{F_{\alpha}(n_1,n_2)}^{\infty}f(x;n_1,n_2)dx=\alpha$的点$F_{\alpha}(n_1,n_2)$为$F(n_1,n_2) $分布的$\alpha$分位数

注意：若$X\sim t(n)$,那么$X^2\sim F(1,n)$

## 正态分布下的抽样分布

设$X_1$,$X_2$,...$X_3$为来自正态总体$N(\mu,\sigma^2)$的简单随机样本，$\overline{X}$是样本均值，$S^2$是样本方差，则有
$$
\overline{X}\sim N(\mu,\frac{\sigma^2}{n})
$$
**定理**：在上述条件下，有

1. $$
   \frac{(n-1)S^2}{\sigma ^2}\sim\chi^2(n-1)
   $$

2. $\overline{X}$与$S^2$相互独立



**注意**

1. $$
   \frac{\sum_{i=1}^{n}(X_i-\overline{X})^2}{\sigma^2}\sim\chi^2(n-1)
   $$

2. $$
   \frac{\sum_{i=1}^{n}(X_i-\mu)^2}{\sigma^2}\sim\chi^2(n)
   $$

   

**定理**：仍然在上述条件下，有
$$
\frac{\overline{X}-\mu}{\frac{S}{\sqrt{n}}}\sim t(n-1)
$$
**定理**:设样本$(X_1,X_2,...X_n)$和$(Y_1,Y_2,...Y_n)$分别来自总体$N(\mu_1,\sigma_1^2)$和$N(\mu_2,\sigma_2^2)$并且他们互相独立，其样本方差分别为$S_1^2$和$S_2^2$,那么有

1. $$
   \frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}\sim F(n_1-1,n_2-1)
   $$

2. $$
   \frac{(\overline{X}-\overline{Y})-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim N(0,1)
   $$

3. 当$\sigma_1^2=\sigma_2^2=\sigma^2$时
   $$
   \frac{(\overline{X}-\overline{Y})-(\mu_1-\mu_2)}{S_w\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim t(n_1+n_2-2)
   $$
   其中
   $$
   S_w^2=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}
   $$

# 参数估计

当总体的参数未知时，需要利用样本对其给出估计，这就是**参数估计**，参数估计有两类方法：**点估计**和**区间估计**

## 参数的点估计

设总体有未知参数$\theta$, $X_1,...X_n$是$X$的简单随机样本。

点估计：构造合适的统计量$\hat{\theta}=\hat{\theta}(X_1,...,X_n)$来估计未知参数$\theta$,$\hat{\theta}$为$\theta$的**点估计量**，当给定样本观察值$x_1,...,x_n$的时候，称$\hat{\theta}(x_1,...,x_n)$为参数$\theta$的**点估计值**

常用的点估计方法：矩法，极大似然法

### 矩估计法

**统计思想**：以样本矩估计总体矩，以样本矩的函数估计总体矩的函数

**理论根据**：辛钦大数定律和依概率收敛的性质

总体$X$的$k$阶矩
$$
\mu_k=E(X^k)
$$
样本$X_1,X_2,...,X_n$的$k$阶矩：
$$
A_k=\frac{1}{n}\sum_{i=1}^{n}X_i^k
$$
由大数定理：
$$
A_k\xrightarrow{P}\mu_k=E(X^k)
$$
解方程$A_k=\mu_k$

**解题步骤**：

1. 先用参数表示均值，或k阶中心距，然后反解参数，用均值或k阶中心距表示参数
2. 将样本的k阶中心距带入上面求得的参数表达式，求得估计参数的值
3. 如果一阶中心距积分为0，那么就求二阶中心距

### 极大似然估计

一般地，设离散型总体 $X\sim p(x;\theta),\theta\in\Theta$, $\theta$未知，从总体$X$取得样本$X_1,X_2,...,X_n$，其观察值为 $x_1,x_2,...x_n$，则事件$\{X_1=x_1,X_2=x_2,...X_n=x_n\}$发生的概率为
$$
L(\theta)=P\{X_1=x_1,..X_n=x_n\}=p(x_1;\theta)...p(x_n;\theta)=\prod_{i=1^n}p(x_i;\theta)
$$
极大似然原理
$$
L(\hat{\theta}(x_1,x_2,...,x_n))=\max_{\theta\in\Theta}L(\theta)
$$
取使得似然函数取得最大值的$\theta$.

对于总体$X$是连续型的情况，似然函数定义为
$$
L(\theta)=\prod_{i=1}^nf(x_i,\theta)
$$
tips:

1. 未知参数可能不止一个，一般设为$\theta=(\theta_1,\theta_2,...,\theta_3)$
2. 求解$L(\theta)$的最大值时，通常取对数运算
3. 如果$L(\theta)$是关于某一个$\theta_i$单调的，此时$\theta_i$的极大似然估计在其边界处取得
4. 若$\hat{\theta}$是$\theta$的极大似然估计，那么$g(\theta)$的极大似然估计为$g(\hat\theta)$

## 估计量的评选准则

对于总体参数的估计，采用不同的估计方法可以求得不同的估计量，对于估计量，有四条评价准则

1. 无偏性准则
2. 有效性准则
3. 均方误差准则
4. 相和型准则

### 无偏性准则

定义：若参数$\theta$的估计量$\hat\theta(X_1,X_2,...,X_n)$满足$E(\hat\theta)=\theta$,则称$\hat\theta$是$\theta$的一个无偏估计量。若$\hat\theta\neq \theta$，那么，$|E(\hat\theta)-\theta|$称为估计量$\hat\theta$的偏差。若$\lim_{n\rightarrow\infty}E(\hat\theta)=\theta$,则称$\hat\theta$是$\theta$的渐进无偏估计量

### 有效性准则

定义：设$\hat\theta_1,\hat\theta_2$是$\theta$的两个无偏估计，如果$D(\hat\theta_1)\leq D(\hat\theta_2)$，对一切$\theta\in \Theta$成立，且不等号至少对某一$\theta\in\Theta$成立，则称$\hat\theta_1$比$\hat\theta_2$有效。

### 均方误差准则

定义：设$\hat\theta$是参数$\theta$的点估计，方差存在，则称 $E(\hat\theta-\theta)^2$是估计量的均方误差，记为$\text{Mse}(\hat\theta)$.
$$
\text{Mse}({\hat\theta})=D(\hat\theta)+(E(\hat\theta)-\theta)^2
$$
若$\hat\theta$是$\theta$的无偏估计，则有
$$
\text{Mse}(\hat\theta)=D(\hat\theta)
$$


### 相合性准则

设$\hat{\theta}(X_1,X_2,..,X_n)$为参数$\theta$的估计量，若对于任意的$\theta\in \Theta$,当$n\rightarrow+\infty$时，$\hat{\theta_n}$依概率收敛为 $\theta$,即$\forall \epsilon>0$,有$\lim_{n\rightarrow+\infty}P\{|\hat{\theta_n}-\theta|\geq \epsilon\}=0$,称$\hat{\theta_n}$为$\theta$的相合估计量或一致估计量。

**例题**

设总体$X\sim U[0,\theta]$,$X_1,...,X_n$是取自$X$的样本，证明$\hat{\theta_1}=2\overline{X}$和$\hat{\theta_2}=\frac{n+1}{n}X_{(n)}$是  $\theta$的相合估计

**证明**
$$
E(\hat{\theta_1})=E(\hat{\theta_2})=\theta,D(\hat{\theta_1})=\frac{\theta^2}{3n},D(\hat{\theta_2})=\frac{\theta^2}{n(n+2)}\\
由切比雪夫不等式，\forall\epsilon>0, 当n\rightarrow+\infty时，\\
有：P\{|\hat{\theta_1}-\theta| \geq \epsilon \}\leq \frac{D(\hat{\theta_1})}{\epsilon^2}\rightarrow 0\\
同理:P\{|\hat{\theta_2}-\theta|\geq \epsilon\}\leq\frac{D(\hat{\theta_2})}{\epsilon^2\rightarrow0}\\
所以\hat{\theta_1}和\hat{\theta_2}都是\theta的相合估计
$$

## 区间估计

假设$(X_1,..X_n)$时总体$X$的一个样本，区间估计的方法是给出两个统计量，$\hat{\theta_1}=\hat{\theta_1}(X_1,...,X_n),\hat{\theta_2}=\hat{\theta_2}(X_1,...,X_n)$使区间$[\hat{\theta_1},\hat{\theta_2}]$以一定的可靠程度覆盖住$\theta$

#### 置信区间的定义

设总体$X$的分布函数$F(x;\theta)$含有一个未知参数$\theta$,$(X_1,..,X_n)$是总体$X$的一个样本，对给定的值$\alpha(0<\alpha<1)$，如果有两个统计量$\hat{\theta_L}=\hat{\theta_L}(X_1,...,X_n),\hat{\theta_U}=\hat{\theta_U}(X_1,...,X_n)$使得
$$
P\{\hat{\theta_L}(X_1,...,X_n)<\theta < \hat{\theta_U}(X_1,...,X_n)\}\geq 1-\alpha,\forall \theta \in \Theta
$$
则称随机区间$\hat{\theta_L},\hat{\theta_U}$是 $\theta$的双侧置信区间，称$1-\alpha$为置信度，$\hat{\theta_L},\hat{\theta_U}$分别为双侧置信下限和双侧置信上限

### 枢轴量法

设总体$X$有概率密度$f(x;\theta)$其中$\theta$是待估计的未知参数，并设$X_1,...,X_n$是来自该总体的样本，称样本和未知参数的函数$G(X_1,...,X_n;\theta)$为枢轴量。

枢轴量和统计量的区别

- 枢轴量是样本和待估参数的函数，其分布不依赖于任何未知参数
- 统计量只是样本的函数，其分布常常依赖于未知参数

##### 构造置信区间的具体步骤

1. 构造一个分布一致的枢轴量$G(X_1,...,X_n;\theta)$

2. 对连续性总体和给定的置信度$1-\alpha$，设常数$a<b$满足
   $$
   P\{a<G(X_1,...,X_n;\theta)<b\}=1-\alpha
   $$

3. 若能从$a<G(X_1,...,X_n;\theta)<b$得到等价的不等式
   $$
   \hat{\theta_L}(X_1,...,X_n)<\theta < \hat{\theta_U}(X_1,...,X_n)
   $$
   那么$(\hat{\theta_L},\hat{\theta_U})$就是$\theta$的置信度为$1-\alpha$的置信区间

枢轴量的构造，通常从参数的点估计$\hat\theta$出发，根据$\hat\theta$ 的分布进行改造而得

##### 正态总体下常见的枢轴量

单个正态总体$N(\mu,\sigma^2)$情形

$\mu$的枢轴量
$$
\begin{cases}
\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}\sim N(0,1),\sigma^2已知\\
\frac{\sqrt{n}(\overline{X}-\mu)}{S}\sim t(n-1),\sigma^2未知
\end{cases}
$$
置信区间分别为
$$
(\overline{X}-\frac{\sigma}{\sqrt{n}}z_{\frac{\alpha}{2}},\overline{X}+\frac{\sigma}{\sqrt{n}}z_{\frac{\alpha}{2}})\\
(\overline{X}-\frac{S}{\sqrt{n}}t_{\frac{\alpha}{2}}(n-1),\overline{X}+\frac{S}{\sqrt{n}}t_{\frac{\alpha}{2}}(n-1))
$$
$\sigma^2$的枢轴量
$$
\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1)
$$
置信区间为
$$
(\frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2}}(n-1)},\frac{(n-1)S^2}{\chi^2_{1-\frac{\alpha}{2}}(n-1)})
$$
两个正态总体$N(\mu_1,\sigma_1^2),N(\mu-2,\sigma_2^2)$

$\mu_1-\mu_2$的枢轴量：
$$
\begin{cases}
\frac{(\overline{X}-\overline{Y})-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim N(0,1),\sigma_1^2,\sigma_2^2已知\\
\frac{(\overline{X}-\overline{Y})-(\mu_1-\mu_2)}{S_w\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim t(n_1+n_2-2),\sigma_1^2=\sigma_2^2未知
\end{cases}
$$
置信区间分别为
$$
((\overline{X}-\overline{Y})\pm z_{\frac{\alpha}{2}}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}})\\
((\overline{X}-\overline{Y})\pm t_{\frac{\alpha}{2}}(n_1+n_2-2)S_w\sqrt{\frac{1}{n_1}+\frac{1}{n_2}})
$$
$\frac{\sigma_1^2}{\sigma_2^2}$的枢轴量
$$
\frac{S_1^2}{S_2^2}/\frac{\sigma_1^2}{\sigma_2^2}\sim F(n_1-1,n_2-1),\mu_1,\mu_2未知
$$

置信区间为
$$
(\frac{S_1^2}{S_2^2}\frac{1}{F_{\frac{\alpha}{2}}(n_1-1,n_2-2)},\frac{S_1^2}{S_2^2}\frac{1}{F_{1-\frac{\alpha}{2}}(n_1-1,n_2-2)})
$$

# 假设检验

统计推断的另一类重要问题的假设检验问题，包括

1. 已知总体分布的形式，需对其中的未知参数给出假设检验（**参数检验**）
2. 总体分布形式完全未知的情况下，对总体的分布或数字特征进行假设检验（**非参数检验**）

**假设**

原假设（零假设）$H_0$，备择假设（对立假设）$H_1$，关于总体参数$\theta$的假设
$$
H_0:\theta\geq \theta_0,H_1:\theta<\theta_0(左边检验)\\
H_0:\theta\leq\theta_0,H_1:\theta>\theta_0(右边检验)\\
H_0:\theta=\theta_0,H_1:\theta\neq\theta_0(双边检验)
$$

#### 检验统计量和拒绝域

如果统计量$T=T(X_1,X_2,...,X_n)$的取值大小和原假设$H_0$是否成立由密切关系，可将之称为对应假设问题的检验统计量，对应于拒绝原假设$H_0$时，样本值的范围称为拒绝域，记为$W$,其补集$\overline{W}$称为接受域

#### 两类错误

由于样本的随机性，任意检验规则在应用时，都有可能发生错误的判断

**第一类错误**：拒绝真实的原假设（弃真）

**第二类错误**：接受错误的原假设（取伪）

犯第一类错误的概率为$\alpha$
$$
\alpha=P\{拒绝H_0|H_0是真的\}
$$
犯第二类错误的概率为 $\beta$
$$
\beta=P\{接受H_0|H_0是假的\}
$$
犯两类错误的概率相互制约，使得在给定样本量 $n$下，不可能找到界值$C$使得$\alpha(C),\beta(C)$都尽可能小

#### `Neyman-Pearson`原则

首先控制犯第一类错误的概率不超过某个常数 $\alpha\in(0,1)$，再寻找检验，使得犯第二类错误的概率尽可能小，其中的常数 $\alpha$称为显著水平，常取$\alpha=0.1,0.05,0.01$等

#### `P_`值与统计显著性

`P_`值：当原假设成立时，检验统计量取比观察到的结果更为极端的数值的概率
$$
P_=P_{H_0}(|\overline{X}-\mu|\geq|\overline{x}-\mu|)
$$
上式表示了`P_`的含义，即抽取一个样本的均值与原假设实际均值的差值大于这一次抽样的均值与原假设实际均值的概率，如果概率很小，那么说明原假设成立即为小概率事件。

`P_`与显著水平$\alpha$的关系：

- 若$P\_\leq\alpha$，等价于样本落在拒绝域内，因此，拒绝原假设，此时称检验结果在水平$\alpha$下是统计显著的
- 若$P\_>\alpha$，等价于样本不落在拒绝域内，因此，不拒绝原假设，此时称检验结果在水平 $\alpha$下是统计不显著的

#### 处理假设检验的基本步骤

1. 根据实际问题提出原假设和备择假设
2. 提出检验统计量和拒绝域的形式
3. 在给定的显著水平 $\alpha$下，根据`Neyman-Pearson`原则求出拒绝域的临界值
4. 根据实际样本观测值做出判断

## 单个正态总体参数的假设检验

设样本$X_1,X_2,...,X_n$来自正态总体$N(\mu,\sigma^2)$，$\overline{X}$和$S^2$分别为样本均值和方差，显著性水平为$\alpha$.

#### 有关均值 $\mu$的检验

1. $\sigma^2$已知时——$Z$检验

   对于双边假设问题$H_0:\mu=\mu_0,H_1:\mu\neq\mu_0$,其中$\mu_0$是已知的常数，取检验统计量为
   $$
   Z=\frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}}
   $$
   在$H_0$为真的条件下$Z\sim N(0,1)$,根据`Neyman-Pearson`原则，检验的拒绝域为
   $$
   W=\{|Z|=|\frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}}|\geq z_{\alpha/2}\}
   $$
   `P_`值的计算：

   对给定的样本观察值 $x_1,x_2,...,x_n$，记检验统计量$Z$的取值为
   $$
   z_0=\frac{\overline{x}-\mu_0}{\sigma/\sqrt{n}}
   $$

   $$
   P\_=P_{H_0}\{|Z|\geq|z_0|\}=2P_{H_0}\{Z\geq|z_0|\}=2(1-\Phi(|z_0|))
   $$

   当`P_`小于显著水平$\alpha$时，拒绝原假设，否则，接受原假设
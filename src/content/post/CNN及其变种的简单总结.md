---
title: "卷积神经网络及其变种的简单总结"
description: "这篇文章简单介绍了卷积神经网络的基本概念、卷积层、池化层、批量归一化等内容，并总结了常见的卷积神经网络架构。"
publishDate: "10 Jan 2024"
updatedDate: "22 Dec 2024"
tags: ["CNN", "deep learning", "neural networks","ai"]
---

**一维情况下的卷积**

假设有一维序列$A$和$B$，$B$的长度比$A$要小，我们如果要计算$B$和$A$的相似程度，我们可以将$B$在$A$上进行滑动，然后一个一个地进行计算相似度（相似度的计算是通过计算两个vector的cosine similarity）

cosine similarity between two vectors x and y:
$$
s=\sum_ix_iy_i
$$
**二维情况下的卷积**

假设有两个二维的图片$A$和$B$，$B$的尺寸比$A$要小，计算$B$和$A$每一部的相似度，我们同样让$B$在$A$上滑动，计算相似度

cosine similarity between two matrices x and y:
$$
s=\sum_{i,j}x_{ij}y_{ij}
$$


**离散情况下的一维卷积公式**（有限长的序列）,f表示长度为$N$的序列，g表示长度为$M$的序列
$$
(f*g)[n]\triangleq\sum_{m=1}^Mf[n-m]g[m]
$$
$f*g$得到的序列长度为$N-M+1$(valid),$M+N-1$(full),$M$(same)

**二维情况下**

假设有两个矩阵$f$和$g$，他们的尺寸分别为$M\times N$和$K_1 \times K_2$,其中$M\geq K_1$,$N\geq K_2$

这两个矩阵的卷积为
$$
h[m,n]=(f*g)[m,n]\triangleq\sum_{k_1=1}^{K_1}\sum_{k_2=1}^{K_2}f[m-k_1,n-k_2]g[k_1,k_2]
$$

`drop out`

好的模型需要对输入数据的扰动具有鲁棒性，丢弃法是在层之间加入噪音。

对$x$加入噪音得到$x_1$满足
$$
x_1=
\begin{cases}
0&\text{with probability p}\\
\frac{x_i}{1-p}&\text{otherwise}
\end{cases}
$$
通常将丢弃法作用于隐藏全连接层的输出上，只在训练的时候进行drop out操作

# 卷积层

**二维卷积层**

- 输入$X$：$n_h\times n_w$

- 核$W$：$k_h\times k_w$

- 偏差$b\in R$

- 输出$Y$：$(n_h-k_h+1)\times(n_w-k_w+1)$
  $$
  Y=X*W+b
  $$

- $W$和$b$是可以学习的参数

输入经过不同的核运算后加上偏置会得到不同的输出效果，我们通过训练可以得到能计算出目标输出的核和偏置

一维输入常用于文本，语言，时序序列

三位输入常用于视频，医学图像，气象地图

# **填充和步幅**

更大的卷积核能更快地减小输出大小，但是如果输入已经比较小了，我们就可以用**填充**来控制输出的大小

**填充**在输入周围添加额外的行列，如果填充$p_h$行和$p_w$列，那么输出的形状就会变为
$$
(n_h-k_h+p_h+q)\times(n_2-k_w+p_w+1)
$$
**步幅**

步幅指的是行列的滑动步长，是为了解决卷积核比较小，输入比较大的时候，要经过很多层才能将输入减小到合适的大小这个问题。

填充和步幅都是卷积层的超参数

# 多输入输出

**多输入通道**

彩色图片一般分为RGB有三个通道

每个通道都有一个卷积核，结果是所有通道卷积结果的和

输入$X$：$c_i\times n_h\times n_w$($c_i$表示通道数)

核$W$：$c_i\times k_h \times k_w$

输出$Y$：$m_h\times m_w$
$$
Y=\sum_{i=0}^{c_i}X_i*W_i
$$
**多输出通道**

还是以彩色图片为例（三通道），我们可以有多个三维卷积核，每个核生成一个输出通道

输入$X$：$c_i\times n_h\times n_w$($c_i$表示通道数)

核$W$：$c_o\times c_i\times k_h \times k_w$($c_o$表示输出通道数)

输出$Y$：$c_o\times m_h\times m_w$
$$
Y_i=X*W_i,i = 1,2,……,c_o
$$
**多个输入和输出通道的作用**

每个输出通道可以识别特定模式

输入通道核识别并组合输出中的模式

$1\times 1$**卷积层**

不识别空间信息，只是进行融合通道。$1\times 1$卷积层能对多个输入的通道进行加权和得到输出。相当于一个全连接层

**总结**

输出通道数是卷积层的超参数，每个输入通道有独立的二维卷积核，所有通道的卷积结果加起来得到一个输出通道的结果，每个输出通道有独立的三维卷积核

# 池化层

卷积对位置敏感，我们需要一定程度的平移不变性，池化层主要就是换就卷积层对位置的敏感性

池化层与卷积层类似，都具有填充和步幅

池化层没有可学习的参数

在每个输入通道应用池化层获得相应的输出通道

输出通道数等于输入通道数

**二维最大池化**

返回滑动窗口的最大值

**平均池化**

返回滑动窗口内的平均值

# 批量归一化

当神经网络很深的时候，损失函数出现在最后，后面的层训练较快，前面的层训练比较慢，前面的层变化会导致所有的层都会变，导致后面的层会需要重新学习多次，导致收敛变慢，如果在学习前面的层的时候避免变化后面的层就能解决这个问题。

- 固定小批量里面的均值和方差，然后再做额外的调整

$$
\mu_B=\frac{1}{|B|}\sum_{i\in B}x_i\\
\sigma_B^{2}=\sum_{i\in B}(x_i-\mu_B)^2+\epsilon
\\x^*=\gamma\frac{x_i-\mu_B}{\sigma}+\lambda
$$

$\gamma$,$\lambda$是用来学习的参数

批量归一化层作用在全连接层和卷积层输出后，激活函数层之前，对于全连接层，作用在特征维，对于卷积层，作用在通道维

批量归一化固定小批量的均值和方差然后学习出适合的偏移和缩放，可以加速收敛速度，但一般不改变模型精度

# LeNet

由两个部分组成：卷积编码器和全连接层密集块

# AlexNet

更大的池化层，更大的核窗口和步长，彩色图片

# VGG

解决“如何更好地让神经网络更深，更大”

使用可重复使用的卷积块来构建深度神经网络，不同的卷积快个数和超参数会影响模型复杂度

一个**VGG块**的特点

- 3$\times$3卷积（填充1）（可以有n层，每一层输入输出都为m个通道）
- 2$\times$2最大池化层（步幅2）

**VGG架构**

- 多个VGG块后接全连接层
- 不同次数的重复块得到不同的架构（VGG-16，VGG-19）

# NiN

解决“卷积层的参数比较少，但是全连接层的参数量很大”

**NiN块**

- 一个卷积层后跟两个全连接层（1$\times$1卷积层）起到全连接层的作用

**NiN架构**

- 无全连接层
- 交替使用NiN块和步幅为2的最大池化层，逐步减小高宽和增大通道数
- 最后使用全局平均池化层得到输出，其输入通道数是类别数

# GoogLeNet

**inception块**

四个路径从不同层面抽取信息，然后在输出通道维合并

1. 1$\times$1卷积层
2. 1$\times$1卷积层+3$\times$3卷积层（pad为1）
3. 1$\times$1卷积层，5$\times$5卷积层（pad为2）
4. 3$\times$3最大池化层（pad为1），1$\times$1卷积层

# ResNet

残差块加入快速通道，使得得到的模型包含原来的的小模型
$$
f(x)=x+g(x)
$$
残差块使得很深的网络更加容易训练